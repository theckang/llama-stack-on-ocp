{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
      "metadata": {},
      "source": [
        "# Level 6: Agents, MCP and RAG \n",
        "\n",
        "This notebook is an extension of the [Level 5 Agentic & MCP notebook](./Level5_agents_and_mcp.ipynb) with the addition of RAG.\n",
        "This tutorial is for developers who are already familiar with [agentic RAG workflows](./Level4_RAG_agent.ipynb). This tutorial will highlight a couple of slightly more advanced use cases for agents where a single tool call is insufficient to complete the required task. Here we will rely on both agentic RAG and MCP server to expand our agents capabilities.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers the following steps:\n",
        "1. Review OpenShift logs for a failing pod.\n",
        "2. Categorize the pod and summarize its error.\n",
        "3. Search available troubleshooting documentations for ideas on how to resolve the error.\n",
        "4. Send a Slack message to the ops team with a brief summary of the error and next steps to take.\n",
        "\n",
        "### MCP Tools:\n",
        "\n",
        "#### OpenShift MCP Server\n",
        "Throughout this notebook we will be relying on the [kubernetes-mcp-server](https://github.com/manusa/kubernetes-mcp-server) by [manusa](https://github.com/manusa) to interact with our OpenShift cluster. Please see installation instructions below if you do not already have this deployed in your environment. \n",
        "\n",
        "* [OpenShift MCP installation instructions](../../../kubernetes/mcp-servers/openshift-mcp/README.md)\n",
        "\n",
        "#### Slack MCP Server\n",
        "We will also be using the [Slack MCP Server](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/slack) in this notebook. Please see installation instructions below if you do not already have this deployed in your environment. \n",
        "\n",
        "* [Slack MCP installation instructions](../../../kubernetes/mcp-servers/slack-mcp/README.md)\n",
        "\n",
        "### Pre-Requisites\n",
        "\n",
        "Before starting, ensure you have the following:\n",
        "- A running Llama Stack server\n",
        "- A running Slack MCP server. Refer to our [documentation](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/slack-mcp) on how you can set this up on your OpenShift cluster\n",
        "- Access to an OpeShift cluster with a deployment of the [OpenShift MCP server](../../../kubernetes/mcp-servers/openshift-mcp) (see the [deployment manifests](https://github.com/opendatahub-io/llama-stack-on-ocp/tree/main/kubernetes/mcp-servers/openshift-mcp) for assistance with this).\n",
        "\n",
        "## Setting Up this Notebook\n",
        "We will initialize our environment as described in detail in our [\\\"Getting Started\\\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fc0a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for communication with Llama Stack\n",
        "from llama_stack_client import LlamaStackClient\n",
        "from llama_stack_client import Agent\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "from llama_stack_client import RAGDocument\n",
        "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
        "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
        "\n",
        "# pretty print of the results returned from the model/agent\n",
        "from termcolor import cprint\n",
        "import sys\n",
        "sys.path.append('..')  \n",
        "from src.utils import step_printer\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08281062-f222-4443-a7bc-0f6166aab36d",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
        "\n",
        "\n",
        "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
        "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
        "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
        "if tavily_search_api_key is None:\n",
        "    provider_data = None\n",
        "else:\n",
        "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
        "\n",
        "\n",
        "client = LlamaStackClient(\n",
        "    base_url=base_url,\n",
        "    provider_data=provider_data\n",
        ")\n",
        "\n",
        "print(f\"Connected to Llama Stack server\")\n",
        "\n",
        "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
        "model_id = \"granite32-8b\"\n",
        "\n",
        "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
        "if temperature > 0.0:\n",
        "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
        "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
        "else:\n",
        "    strategy = {\"type\": \"greedy\"}\n",
        "\n",
        "max_tokens = int(os.getenv(\"MAX_TOKENS\", 512))\n",
        "\n",
        "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
        "sampling_params = {\n",
        "    \"strategy\": strategy,\n",
        "    \"max_tokens\": max_tokens,\n",
        "}\n",
        "\n",
        "# For this demo, we are using Milvus Lite, which is our preferred solution. Any other Vector DB supported by Llama Stack can be used.\n",
        "\n",
        "# RAG vector DB settings\n",
        "VECTOR_DB_EMBEDDING_MODEL = os.getenv(\"VDB_EMBEDDING\")\n",
        "VECTOR_DB_EMBEDDING_DIMENSION = int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384))\n",
        "VECTOR_DB_CHUNK_SIZE = int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512))\n",
        "VECTOR_DB_PROVIDER_ID = os.getenv(\"VDB_PROVIDER\")\n",
        "\n",
        "# Unique DB ID for session\n",
        "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
        "\n",
        "stream_env = os.getenv(\"STREAM\", \"False\")\n",
        "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
        "# any value non equal to 'False' will be considered as 'True'\n",
        "stream = (stream_env != \"False\")\n",
        "\n",
        "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66044170",
      "metadata": {},
      "source": [
        "### Validate tools are available in our Llama Stack instance\n",
        "\n",
        "We will be using the [OpenShift MCP Server](https://github.com/manusa/kubernetes-mcp-server) and the [Slack MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/slack) for performing this task. If you haven't already, you can follow the instructions [here](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/slack-mcp#setting-up-on-ocp) to install the Slack MCP server and the instructions [here](https://github.com/opendatahub-io/llama-stack-demos/tree/main/kubernetes/mcp-servers/openshift-mcp#steps-for-deploying-the-openshift-mcp-server-on-openshift) to install the OpenShift MCP server. Once we confirm that the OpenShift and Slack MCP servers are running and configured to your Llama Stack server, we can then move on to defining an agent to help with our task.\n",
        "\n",
        "#### Registering tools on Llama Stack\n",
        "When an instance of llama stack is redeployed your tools need to re-registered. Also if a tool is already registered with a llama stack instance, if you try to register one with the same `toolgroup_id`, llama stack will throw you an error.\n",
        "\n",
        "For this reason it is recommended to include some code to validate your tools and toolgroups. This is where the `mcp_url` comes into play. The following code will check that the `builtin::rag`,`mcp::openshift`  and `mcp::slack` tools are registered as tools, but if any mcp tool is not listed there, it will attempt to register it using the mcp url.\n",
        "\n",
        "If you are running the MCP server from source, the default value for this is: `http://localhost:8000/sse`.\n",
        "\n",
        "If you are running the MCP server from a container, the default value for this is: `http://host.containers.internal:8000/sse`.\n",
        "\n",
        "Make sure to pass the corresponding MCP URL for the server you are trying to register/validate tools for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Enter your MCP server URL here\n",
        "ocp_mcp_url = os.getenv(\"REMOTE_OCP_MCP_URL\") # Optional: enter your MCP server url here\n",
        "slack_mcp_url = os.getenv(\"REMOTE_SLACK_MCP_URL\") # Optional: enter your MCP server url here\n",
        "\n",
        "# Get list of registered tools and extract their toolgroup IDs\n",
        "registered_tools = client.tools.list()\n",
        "registered_toolgroups = [tool.toolgroup_id for tool in registered_tools]\n",
        "\n",
        "if  \"builtin::rag\" not in registered_toolgroups: # Required\n",
        "    client.toolgroups.register(\n",
        "        toolgroup_id=\"builtin::rag\",\n",
        "        provider_id=\"milvus\"\n",
        "    )\n",
        "\n",
        "if \"mcp::openshift\" not in registered_toolgroups: # required\n",
        "    client.toolgroups.register(\n",
        "        toolgroup_id=\"mcp::openshift\",\n",
        "        provider_id=\"model-context-protocol\",\n",
        "        mcp_endpoint={\"uri\":ocp_mcp_url},\n",
        "    )\n",
        "\n",
        "if \"mcp::slack\" not in registered_toolgroups: # required\n",
        "    client.toolgroups.register(\n",
        "        toolgroup_id=\"mcp::slack\",\n",
        "        provider_id=\"model-context-protocol\",\n",
        "        mcp_endpoint={\"uri\":slack_mcp_url},\n",
        "    )\n",
        "\n",
        "# Log the current toolgroups registered\n",
        "print(f\"Your Llama Stack server is already registered with the following tool groups: {set(registered_toolgroups)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc8d3f8",
      "metadata": {},
      "source": [
        "## Discover and report solutions to errors that might be happening on my OpenShift Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f6993e",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "We will now try to use an agent to:\n",
        "- **Retrieve the logs of a failing pod running on OpenShift**\n",
        "- **Use RAG to find solutions to the errors in the pod logs based on OpenShift troubleshooting documents**\n",
        "- **Send a summary of the solution as a message on Slack.**\n",
        "\n",
        "### Pod with simulated error logs\n",
        "\n",
        "**Important** - In the [Level5 Agents and MCP notebook](./Level5_agents_and_mcp.ipynb) we deployed a pod called `slack-test` on an OpenShift cluster that generates simulated error logs. For the purpose of testing the pod log retrieval, we will be using that pod for this notebook and assume that it is already running on your cluster. **If you haven't already, please make sure to go through the Level5 notebook before continuing this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85664f8",
      "metadata": {},
      "source": [
        "### RAG: Indexing the Documents\n",
        "\n",
        "We will be leveraging RAG to help us find solutions from OpenShift troubleshooting documents for debugging the logs of erroneous pods running on OpenShift. We will need to:\n",
        "- Initialize a new document collection in the target vector DB. All parameters related to the vector DB, such as the embedding model and dimension, must be specified here.\n",
        "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, conversion and chunking of the documents' content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a8f4dc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define and register the document collection to be used\n",
        "client.vector_dbs.register(\n",
        "    vector_db_id=vector_db_id,\n",
        "    embedding_model=VECTOR_DB_EMBEDDING_MODEL,\n",
        "    embedding_dimension=VECTOR_DB_EMBEDDING_DIMENSION,\n",
        "    provider_id=VECTOR_DB_PROVIDER_ID,\n",
        ")\n",
        "\n",
        "# ingest the documents into the newly created document collection\n",
        "urls = [\n",
        "    (\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.11/pdf/support/OpenShift_Container_Platform-4.11-Support-en-US.pdf\", \"application/pdf\"),\n",
        "]\n",
        "documents = [\n",
        "    RAGDocument(\n",
        "        document_id=f\"num-{i}\",\n",
        "        content=url,\n",
        "        mime_type=url_type,\n",
        "        metadata={},\n",
        "    )\n",
        "    for i, (url, url_type) in enumerate(urls)\n",
        "]\n",
        "client.tool_runtime.rag_tool.insert(\n",
        "    documents=documents,\n",
        "    vector_db_id=vector_db_id,\n",
        "    chunk_size_in_tokens=VECTOR_DB_CHUNK_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf70ace-b704-4379-aa88-3c793cc4f959",
      "metadata": {},
      "source": [
        "### System Prompt for LLM model\n",
        "\n",
        "**Note:** If you have multiple models configured with your Llama Stack server, you can choose which one to run your queries against. When switching to a different model, you may need to adjust the system prompt to align with that modelâ€™s expected behavior. Many models provide recommended system prompts for optimal and reliable outputs these are typically documented on their respective websites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3136e6dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_prompt= \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
        "Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b675f2a",
      "metadata": {},
      "source": [
        "## Defining our Agent - Prompt Chaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b340e517",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple agent with tools\n",
        "agent = Agent(\n",
        "    client,\n",
        "    model=model_id, # replace this with your choice of model\n",
        "    instructions = model_prompt , # update system prompt based on the model you are using\n",
        "    tools=[dict(\n",
        "            name=\"builtin::rag\",\n",
        "            args={\n",
        "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
        "            },\n",
        "        ),\"mcp::openshift\", \"mcp::slack\"],\n",
        "    tool_config={\"tool_choice\":\"auto\"},\n",
        "    sampling_params=sampling_params\n",
        ")\n",
        "\n",
        "user_prompts = [\"View the logs for pod slack-test in the llama-serve OpenShift namespace. Categorize it as normal or error.\",\n",
        "                \"Search for solutions on this error and provide a summary of the steps to take in just 1-2 sentences.\",\n",
        "                \"Send a message with the summarization to the demos channel on Slack with a tool call\"]\n",
        "session_id = agent.create_session(session_name=\"OCP_Slack_demo\")\n",
        "for i, prompt in enumerate(user_prompts):\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\":\"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream,\n",
        "    )\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a105531",
      "metadata": {},
      "source": [
        "### Output Analysis\n",
        "\n",
        "Lets step through the output to further understands whats happening in this Agentic demo.\n",
        "\n",
        "1. First the LLM sends off a tool call to the `pods_log` tool configured with the OpenShift MCP server, to fetch the logs for the pod specified from the OpenShift cluster.\n",
        "2. The tool successfully retrieves the logs for the pod.\n",
        "3. The LLM recieves the response from the tool call, which are the pod logs, along with the original query and using its own knowledge tries to categorize it as 'Normal or 'Error'.\n",
        "4. Next, the LLM sends a tool call to the RAG tool, to query the vector DB on the documents for the error from the pod logs and look for a solution to resolve the error from the document.\n",
        "5. The LLM recieves the response from the tool call, and summarizes the information it retreived from the document.\n",
        "6. Finally, the LLM sends the summarization as a message on Slack using the `slack_post_message` tool call configured with the Slack MCP server."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f6c84d",
      "metadata": {},
      "source": [
        "## Defining our Agent - ReAct\n",
        "\n",
        "Now that we've shown that we can successfully accomplish this multi-step multi-tool task using prompt chaining, let's see if we can give our agent a bit more autonomy to perform the same task but with a single prompt instead of a chian. To do this, we will instantiate a ReAct agent (which is included in the llama stack python client by default). The ReAct agent is a variant of the simple agent but with the ability to loop through \"Reason then Act\" iterations, thinking through the problem and then using tools until it determines that it's task has been completed successfully.\n",
        "\n",
        "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively.\n",
        "\n",
        "Below you will see the slight differences in the agent definition and the prompt used to accomplish our task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a606a5e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "builtin_rag = dict(\n",
        "            name=\"builtin::rag\",\n",
        "            args={\n",
        "                \"vector_db_ids\": [vector_db_id],  # list of IDs of document collections to consider during retrieval\n",
        "            },\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "agent = ReActAgent(\n",
        "            client=client,\n",
        "            model=model_id,\n",
        "            tools=[\"mcp::slack\",\"mcp::openshift\", builtin_rag],\n",
        "            response_format={\n",
        "                \"type\": \"json_schema\",\n",
        "                \"json_schema\": ReActOutput.model_json_schema(),\n",
        "            },\n",
        "            sampling_params={\"max_tokens\":512},\n",
        "        )\n",
        "\n",
        "user_prompts = [\"\"\"View the logs for pod slack-test in the llama-serve OpenShift namespace. Categorize it as normal or error, \n",
        "                search for solutions on this error and provide a summary of the steps to take in just 1-2 sentences.\n",
        "                Send a message with the summarization to the demos channel on Slack. Let me know when you've sent the message.\n",
        "                \"\"\"\n",
        "                ]\n",
        "\n",
        "session_id = agent.create_session(\"web-session\")\n",
        "for prompt in user_prompts:\n",
        "    print(\"\\n\"+\"=\"*50)\n",
        "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
        "    print(\"=\"*50)\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=stream\n",
        "    )\n",
        "    if stream:\n",
        "        for log in EventLogger().log(response):\n",
        "            log.print()\n",
        "    else:\n",
        "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f4147a",
      "metadata": {},
      "source": [
        "### Output Analysis\n",
        "\n",
        "Above, we can see that the ReAct agent took nearly an identical approach to the prompt chaining method above, but using a single prompt instead of a chain.\n",
        "\n",
        "1. First, the agent reasoned that it needed to generated a tool call for the `pods_log` tool included in the OpenShift MCP server and did so to generate a tool call that fetched the logs for the specified pod.\n",
        "2. Next, the agent reasoned that its next step was to query the available vector database for more information about the logs and acted using the built-in `knowledge_search` tool. \n",
        "4. Then, the agent summarized the findings from the the knowledge search step and reasoned it needed to send a slack message to share this information with the team and did so by using the `slack_post_message` tool call configured with the Slack MCP server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3105c368",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "This tutorial demonstrates how to implement agentic RAG and MCP applications with Llama Stack. We do so by initializing an agent while giving it access to the MCP tools, and RAG tool configured with Llama Stack, then invoking the agent on the specified query. We showed that we can do this with more directed Prompt Chaining or with the more open ended ReAct pattern Please check out our other notebooks for more examples using Llama Stack."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d991b679",
      "metadata": {},
      "source": [
        "#### Any Feedback?\n",
        "\n",
        "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
